{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading & first Pre-Processing steps\n",
    "import numpy as np\n",
    "\n",
    "#Read data\n",
    "\n",
    "X = []\n",
    "samples_x = []\n",
    "\n",
    "f = open(\"genotypes.csv\",\"r\")\n",
    "\n",
    "for i,line in enumerate(f):\n",
    "    if(i == 0):\n",
    "       continue\n",
    "    \n",
    "    sv = line.strip().split(\",\")\n",
    "    samples_x.append(sv[0])\n",
    "    \n",
    "    row = []\n",
    "    for element in sv[1:]:\n",
    "        row.append(element)\n",
    "    X.append(row)\n",
    "    \n",
    "f.close()\n",
    "\n",
    "X = np.array(X)\n",
    "samples_x = np.array(samples_x)\n",
    "\n",
    "y = []\n",
    "samples_y = []\n",
    "\n",
    "f = open(\"phenotype_values.csv\",\"r\")\n",
    "\n",
    "for i,line in enumerate(f):\n",
    "    if(i == 0):\n",
    "       continue\n",
    "    sv = line.strip().split(\",\")\n",
    "    samples_y.append(sv[0])\n",
    "    y.append(float(sv[1]))\n",
    "    \n",
    "f.close()\n",
    "\n",
    "y = np.array(y)\n",
    "samples_y = np.array(samples_y)\n",
    "\n",
    "#Replace \"?\" with np.nan\n",
    "X = np.where(X == \"?\", np.nan, X)\n",
    "\n",
    "# Match genotypes with phenotype values knn\n",
    "truth_table = (samples_x[:,np.newaxis]==samples_y)\n",
    "ind = np.where(truth_table==True)\n",
    "\n",
    "samples_x = samples_x[ind[0]]\n",
    "samples_y = samples_y[ind[1]]\n",
    "\n",
    "X = X[ind[0],:]\n",
    "y = y[ind[1]]\n",
    "\n",
    "# Split into train data and test data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode nucleotide values\n",
    "\n",
    "for j in range (X_train.shape[1]):\n",
    "    \n",
    "    values, counts = np.unique(X_train[:,j], return_counts=True)\n",
    "    values=values[:2]\n",
    "    counts=counts[:2]\n",
    "\n",
    "    X_train[:,j] = np.where(X_train[:,j] == values[np.argmax(counts)], 0, X_train[:,j])\n",
    "    X_train[:,j] = np.where(X_train[:,j] == values[np.argmin(counts)], 2, X_train[:,j])\n",
    "    #Encode test data based on the known data\n",
    "    X_test[:,j] = np.where(X_test[:,j] == values[np.argmax(counts)], 0, X_test[:,j]) \n",
    "    X_test[:,j] = np.where(X_test[:,j] == values[np.argmin(counts)], 2, X_test[:,j])\n",
    "    \n",
    "#Convert array from string to float\n",
    "X_train = X_train.astype(np.float_)\n",
    "X_test = X_test.astype(np.float_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values on training data with KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn_imputer = KNNImputer (missing_values=np.nan, n_neighbors=3, weights=\"distance\")\n",
    "X_train = knn_imputer.fit_transform(X_train)\n",
    "\n",
    "for i in range (X_test.shape[0]):\n",
    "    x = X_test[i]\n",
    "    x.shape = (1,5000)\n",
    "    X_testr = knn_imputer.fit_transform(np.append(x,X_train,axis = 0))\n",
    "    [test_data, R] = np.split(X_testr,[1],axis = 0)\n",
    "    while i == 0:\n",
    "        test = test_data\n",
    "        break\n",
    "    while i >= 1:\n",
    "        test = np.append(test,test_data,axis = 0)\n",
    "        break\n",
    "\n",
    "for j in range (X_train.shape[1]):\n",
    "    X_train[:,j] = np.where(X_train[:,j] < 1, 0.0, X_train[:,j])\n",
    "    X_train[:,j] = np.where(X_train[:,j] >= 1, 2.0, X_train[:,j])\n",
    "    test[:,j] = np.where(test[:,j] < 1, 0.0, test[:,j])\n",
    "    test[:,j] = np.where(test[:,j] >= 1, 2.0, test[:,j])\n",
    "    X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA was not always used please see the poster for replication\n",
    "\n",
    "#standardise the data\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xn = scaler.fit_transform(X_train)\n",
    "\n",
    "#compute the covariance matrix on the standardised data\n",
    "n = Xn.shape[0]\n",
    "C = 1/(n-1) * np.dot(Xn.T,Xn)\n",
    "\n",
    "#eigendecompose the matrix  ùêÇ  into its eigenvalues and eigenvector\n",
    "import numpy.linalg as linalg\n",
    "d, V = linalg.eig(C)\n",
    "\n",
    "#sort the eigenvalues in decreasing order, re-sort the columns of the eigenvector matrix ùëâ  using the indices from the sorting of the eigenvalues\n",
    "ind = np.argsort(d)[::-1]\n",
    "d = d[ind]\n",
    "V = V[:,ind]\n",
    "\n",
    "Xn1 = np.dot(Xn,V[:,0:750])\n",
    "X_train = np.array(Xn1,dtype=float)\n",
    "\n",
    "#calculate how much of the total variance the PCs account for\n",
    "ratios_variance_explained = d/d.sum()\n",
    "va = ratios_variance_explained[0:750].sum()\n",
    "\n",
    "#transform test data\n",
    "Xn3_Xtest = np.dot(test,V[:,0:750])\n",
    "X_test = np.array(Xn3_Xtest,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###MODELS###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy.linalg as linalg\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "K = np.arange(1,len(X_train)+1,1)\n",
    "r2_trainknr = []\n",
    "r2_testknr = []\n",
    "\n",
    "start = time.process_time()\n",
    "for k in K:\n",
    "    knr = KNeighborsRegressor(n_neighbors=k)\n",
    "    knr.fit(X_train,y_train)\n",
    "    knr_predictions_training = knr.predict(X_train)\n",
    "    knr_predictions_testing = knr.predict(X_test)\n",
    "    r2_trainingknr = metrics.r2_score(y_train,knr_predictions_training)\n",
    "    r2_testingknr = metrics.r2_score(y_test,knr_predictions_testing)\n",
    "    r2_trainknr.append(r2_trainingknr)\n",
    "    r2_testknr.append(r2_testingknr)\n",
    "\n",
    "best_k = K[np.argmax(r2_testknr)]\n",
    "rint(\"Best_k = \" + str(best_k))\n",
    "print(\"Best R2: %.4f\" %np.max(r2_testknr))\n",
    "print(r2_testknr)\n",
    "\n",
    "#____________________________________________________________\n",
    "\n",
    "delta = (time.process_time()-start)\n",
    "print(\"Computation Time: %f s\" %delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNetCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy.linalg as linalg\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.impute import KNNImputer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "start = time.process_time()\n",
    "l1r = np.arange(0.1,1.05,0.05)\n",
    "\n",
    "#ElasticNetCV    \n",
    "elncv = linear_model.ElasticNetCV(cv = 5, l1_ratio=l1r, max_iter=5000, random_state=73)\n",
    "elncv.fit(X_train, y_train)\n",
    "predictions_training = elncv.predict(X_train)\n",
    "predictions_testing = elncv.predict(test)\n",
    "\n",
    "print(\"MSE (Train):\\t%.2f\" % metrics.mean_squared_error(y_train, predictions_training))\n",
    "print(\"R2 (Train):\\t%.2f\" % metrics.r2_score(y_train, predictions_training))\n",
    "print(\"MSE (Test):\\t%.2f\" % metrics.mean_squared_error(y_test, predictions_testing))\n",
    "print(\"R2 (Test):\\t%.2f\" % metrics.r2_score(y_test, predictions_testing))\n",
    "print(\"The selected Alpha:\\t%.f \" %elncv.alpha_)\n",
    "print(\"The selected L1-Ration:\\t%.f \" %elncv.l1_ratio_)\n",
    "\n",
    "delta = time.process_time() - start\n",
    "print(\"Computation Time: %f s\" %delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Train):\t0.65\n",
      "R2 (Train):\t0.94\n",
      "MSE (Test):\t3.41\n",
      "R2 (Test):\t0.66\n",
      "Computation Time: 1209.324586 s\n"
     ]
    }
   ],
   "source": [
    "#ElasticNet Model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "eln =ElasticNet(alpha=0.05792901004687936, l1_ratio=0.25, max_iter=7000, random_state=73)\n",
    "eln.fit(X_train, y_train)\n",
    "predictions_training = eln.predict(X_train)\n",
    "predictions_testing = eln.predict(X_test)\n",
    "\n",
    "print(\"MSE (Train):\\t%.2f\" % metrics.mean_squared_error(y_train, predictions_training))\n",
    "print(\"R2 (Train):\\t%.2f\" % metrics.r2_score(y_train, predictions_training))\n",
    "print(\"MSE (Test):\\t%.2f\" % metrics.mean_squared_error(y_test, predictions_testing))\n",
    "print(\"R2 (Test):\\t%.2f\" % metrics.r2_score(y_test, predictions_testing))\n",
    "\n",
    "#____________________________________________________________\n",
    "\n",
    "delta = (time.process_time()-start)\n",
    "print(\"Computation Time: %f s\" %delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
