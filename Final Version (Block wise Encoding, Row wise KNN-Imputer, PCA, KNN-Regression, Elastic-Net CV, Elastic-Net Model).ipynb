{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading & first Pre-Processing steps\n",
    "import numpy as np\n",
    "\n",
    "#Read data\n",
    "\n",
    "X = []\n",
    "samples_x = []\n",
    "\n",
    "f = open(\"genotypes.csv\",\"r\")\n",
    "\n",
    "for i,line in enumerate(f):\n",
    "    if(i == 0):\n",
    "       continue\n",
    "    \n",
    "    sv = line.strip().split(\",\")\n",
    "    samples_x.append(sv[0])\n",
    "    \n",
    "    row = []\n",
    "    for element in sv[1:]:\n",
    "        row.append(element)\n",
    "    X.append(row)\n",
    "    \n",
    "f.close()\n",
    "\n",
    "X = np.array(X)\n",
    "samples_x = np.array(samples_x)\n",
    "\n",
    "y = []\n",
    "samples_y = []\n",
    "\n",
    "f = open(\"phenotype_values.csv\",\"r\")\n",
    "\n",
    "for i,line in enumerate(f):\n",
    "    if(i == 0):\n",
    "       continue\n",
    "    sv = line.strip().split(\",\")\n",
    "    samples_y.append(sv[0])\n",
    "    y.append(float(sv[1]))\n",
    "    \n",
    "f.close()\n",
    "\n",
    "y = np.array(y)\n",
    "samples_y = np.array(samples_y)\n",
    "\n",
    "#Replace \"?\" with np.nan\n",
    "X = np.where(X == \"?\", np.nan, X)\n",
    "\n",
    "# Match genotypes with phenotype values knn\n",
    "truth_table = (samples_x[:,np.newaxis]==samples_y)\n",
    "ind = np.where(truth_table==True)\n",
    "\n",
    "samples_x = samples_x[ind[0]]\n",
    "samples_y = samples_y[ind[1]]\n",
    "\n",
    "X = X[ind[0],:]\n",
    "y = y[ind[1]]\n",
    "\n",
    "# Split into train data and validation data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.2, random_state=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode nucleotide values\n",
    "\n",
    "for j in range (X_train.shape[1]):\n",
    "    \n",
    "    values, counts = np.unique(X_train[:,j], return_counts=True)\n",
    "    values = values[:2]\n",
    "    counts = counts[:2]\n",
    "\n",
    "    X_train[:,j] = np.where(X_train[:,j] == values[np.argmax(counts)], 0, X_train[:,j])\n",
    "    X_train[:,j] = np.where(X_train[:,j] == values[np.argmin(counts)], 2, X_train[:,j])\n",
    "    #Encode validation data based on the known data\n",
    "    X_val[:,j] = np.where(X_val[:,j] == values[np.argmax(counts)], 0, X_val[:,j]) \n",
    "    X_val[:,j] = np.where(X_val[:,j] == values[np.argmin(counts)], 2, X_val[:,j])\n",
    "    \n",
    "#Convert array from string to float\n",
    "X_train = X_train.astype(np.float_)\n",
    "X_val = X_val.astype(np.float_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values on training data with KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn_imputer = KNNImputer (missing_values=np.nan, n_neighbors=3, weights=\"distance\")\n",
    "X_train = knn_imputer.fit_transform(X_train)\n",
    "\n",
    "for i in range (X_val.shape[0]):\n",
    "    x = X_val[i]\n",
    "    x.shape = (1,5000)\n",
    "    X_valr = knn_imputer.fit_transform(np.append(x,X_train,axis = 0))\n",
    "    [val_data, R] = np.split(X_valr,[1],axis = 0)\n",
    "    while i == 0:\n",
    "        val = val_data\n",
    "        break\n",
    "    while i >= 1:\n",
    "        val = np.append(val,val_data,axis = 0)\n",
    "        break\n",
    "\n",
    "for j in range (X_train.shape[1]):\n",
    "    X_train[:,j] = np.where(X_train[:,j] < 1, 0.0, X_train[:,j])\n",
    "    X_train[:,j] = np.where(X_train[:,j] >= 1, 2.0, X_train[:,j])\n",
    "    val[:,j] = np.where(val[:,j] < 1, 0.0, val[:,j])\n",
    "    val[:,j] = np.where(val[:,j] >= 1, 2.0, val[:,j])\n",
    "    X_val = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA was not always used please see the poster for replication\n",
    "\n",
    "#standardise the data\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xn = scaler.fit_transform(X_train)\n",
    "\n",
    "#compute the covariance matrix on the standardised data\n",
    "n = Xn.shape[0]\n",
    "C = 1/(n-1) * np.dot(Xn.T,Xn)\n",
    "\n",
    "#eigendecompose the matrix  ùêÇ  into its eigenvalues and eigenvector\n",
    "import numpy.linalg as linalg\n",
    "d, V = linalg.eig(C)\n",
    "\n",
    "#sort the eigenvalues in decreasing order, re-sort the columns of the eigenvector matrix ùëâ  using the indices from the sorting of the eigenvalues\n",
    "ind = np.argsort(d)[::-1]\n",
    "d = d[ind]\n",
    "V = V[:,ind]\n",
    "\n",
    "Xn1 = np.dot(Xn,V[:,0:750])\n",
    "X_train = np.array(Xn1,dtype=float)\n",
    "\n",
    "#calculate how much of the total variance the PCs account for\n",
    "ratios_variance_explained = d/d.sum()\n",
    "va = ratios_variance_explained[0:750].sum()\n",
    "\n",
    "#transform validation data\n",
    "Xn3_Xval = np.dot(val,V[:,0:750])\n",
    "X_val = np.array(Xn3_Xval,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###MODELS###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Regression k-validation\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "\n",
    "K = np.arange(5,26,1)\n",
    "r2_trainknr = []\n",
    "r2_valknr = []\n",
    "\n",
    "start = time.process_time()\n",
    "for k in K:\n",
    "    knr = KNeighborsRegressor(n_neighbors=k)\n",
    "    knr.fit(X_train,y_train)\n",
    "    knr_predictions_training = knr.predict(X_train)\n",
    "    knr_predictions_validating = knr.predict(X_val)\n",
    "    r2_trainingknr = metrics.r2_score(y_train,knr_predictions_training)\n",
    "    r2_validationknr = metrics.r2_score(y_val,knr_predictions_validating)\n",
    "    r2_trainknr.append(r2_trainingknr)\n",
    "    r2_valknr.append(r2_validationknr)\n",
    "\n",
    "best_k = K[np.argmax(r2_valknr)]\n",
    "rint(\"Best_k = \" + str(best_k))\n",
    "print(\"Best R2: %.4f\" %np.max(r2_valknr))\n",
    "\n",
    "delta = (time.process_time()-start)\n",
    "print(\"Computation Time: %f s\" %delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNetCV\n",
    "from sklearn import linear_model\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "\n",
    "\n",
    "start = time.process_time()\n",
    "l1r = np.arange(0.1,1.05,0.05)\n",
    "\n",
    "#ElasticNetCV    \n",
    "elncv = linear_model.ElasticNetCV(cv = 5, l1_ratio=l1r, max_iter=7000, random_state=73)\n",
    "elncv.fit(X_train, y_train)\n",
    "predictions_training = elncv.predict(X_train)\n",
    "predictions_validating = elncv.predict(val)\n",
    "\n",
    "print(\"MSE (Train):\\t%.2f\" % metrics.mean_squared_error(y_train, predictions_training))\n",
    "print(\"R2 (Train):\\t%.2f\" % metrics.r2_score(y_train, predictions_training))\n",
    "print(\"MSE (Val):\\t%.2f\" % metrics.mean_squared_error(y_val, predictions_validating))\n",
    "print(\"R2 (Val):\\t%.2f\" % metrics.r2_score(y_val, predictions_validating))\n",
    "print(\"The selected Alpha:\\t%.f \" %elncv.alpha_)\n",
    "print(\"The selected L1-Ration:\\t%.f \" %elncv.l1_ratio_)\n",
    "\n",
    "delta = time.process_time() - start\n",
    "print(\"Computation Time: %f s\" %delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Regression Model based on split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "knr = KNeighborsRegressor(n_neighbors=7)\n",
    "knr.fit(X_train,y_train)\n",
    "knr_predictions_training = knr.predict(X_train)\n",
    "knr_predictions_validating = knr.predict(X_val)\n",
    "\n",
    "print(\"MSE (Train):\\t%.2f\" % metrics.mean_squared_error(y_train, knr_predictions_training))\n",
    "print(\"R2 (Train):\\t%.2f\" % metrics.r2_score(y_train, knr_predictions_training))\n",
    "print(\"MSE (Val):\\t%.2f\" % metrics.mean_squared_error(y_val, knr_predictions_validating))\n",
    "print(\"R2 (Val):\\t%.2f\" % metrics.r2_score(y_val, knr_predictions_validating))\n",
    "\n",
    "delta = time.process_time() - start\n",
    "print(\"Computation Time: %f s\" %delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNet Model based on split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "eln =ElasticNet(alpha=0.05792901004687936, l1_ratio=0.25, max_iter=7000, random_state=73)\n",
    "eln.fit(X_train, y_train)\n",
    "predictions_training = eln.predict(X_train)\n",
    "predictions_validating = eln.predict(X_val)\n",
    "\n",
    "print(\"MSE (Train):\\t%.2f\" % metrics.mean_squared_error(y_train, predictions_training))\n",
    "print(\"R2 (Train):\\t%.2f\" % metrics.r2_score(y_train, predictions_training))\n",
    "print(\"MSE (Val):\\t%.2f\" % metrics.mean_squared_error(y_val, predictions_validating))\n",
    "print(\"R2 (Val):\\t%.2f\" % metrics.r2_score(y_val, predictions_validating))\n",
    "\n",
    "delta = (time.process_time()-start)\n",
    "print(\"Computation Time: %f s\" %delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREDICTING UNKNOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Read known genotype values\n",
    "X_train = []\n",
    "samples_x_train = []\n",
    "\n",
    "f = open(\"genotypes.csv\",\"r\")\n",
    "\n",
    "for i,line in enumerate(f):\n",
    "    if(i == 0):\n",
    "        continue\n",
    "    \n",
    "    sv = line.strip().split(\",\")\n",
    "    samples_x_train.append(sv[0])\n",
    "    \n",
    "    row = []\n",
    "    for element in sv[1:]:\n",
    "        row.append(element)\n",
    "    X_train.append(row)\n",
    "    \n",
    "f.close()\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "samples_x_train = np.array(samples_x_train)\n",
    "\n",
    "# Read phenotype values\n",
    "\n",
    "y_train = []\n",
    "samples_y_train = []\n",
    "\n",
    "f = open(\"phenotype_values.csv\",\"r\")\n",
    "\n",
    "for i,line in enumerate(f):\n",
    "    if(i == 0):\n",
    "        continue\n",
    "    sv = line.strip().split(\",\")\n",
    "    samples_y_train.append(sv[0])\n",
    "    y_train.append(float(sv[1]))\n",
    "    \n",
    "f.close()\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "samples_y_train = np.array(samples_y_train)\n",
    "\n",
    "#Read unknown genotypes\n",
    "X_test = []\n",
    "samples_x_test=[]\n",
    "\n",
    "fd=open(\"unknown_genotypes.csv\",\"r\")\n",
    "\n",
    "for i, line in enumerate(fd):\n",
    "    if(i==0):\n",
    "        continue\n",
    "    sv=line.strip().split(\",\")\n",
    "    samples_x_test.append(sv[0])\n",
    "    row=[]\n",
    "    \n",
    "    for element in sv[1:]:\n",
    "        row.append(element)\n",
    "    X_test.append(row)\n",
    "    \n",
    "fd.close()\n",
    "X_test=np.array(X_test)\n",
    "samples_x_test=np.array(samples_x_test)\n",
    "\n",
    "#Replace \"?\" with np.nan\n",
    "X_train = np.where(X_train == \"?\", np.nan, X_train)\n",
    "X_test = np.where(X_test == \"?\", np.nan, X_test)\n",
    "\n",
    "#Match genotypes with phenotype values\n",
    "truth_table = (samples_x_train[:,np.newaxis]==samples_y_train)\n",
    "ind = np.where(truth_table==True)\n",
    "\n",
    "samples_x_train = samples_x_train[ind[0]]\n",
    "samples_y_train = samples_y_train[ind[1]]\n",
    "\n",
    "X_train = X_train[ind[0],:]\n",
    "y_train = y_train[ind[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode nucleotide values\n",
    "for j in range (X_train.shape[1]):\n",
    "    \n",
    "    values, counts = np.unique(X_train[:,j], return_counts=True)\n",
    "    values=values[:2]\n",
    "    counts=counts[:2]\n",
    "\n",
    "    X_train[:,j] = np.where(X_train[:,j] == values[np.argmax(counts)], 0, X_train[:,j])\n",
    "    X_train[:,j] = np.where(X_train[:,j] == values[np.argmin(counts)], 2, X_train[:,j])\n",
    "    X_test[:,j] = np.where(X_test[:,j] == values[np.argmax(counts)], 0, X_test[:,j]) #Encode based on the known data\n",
    "    X_test[:,j] = np.where(X_test[:,j] == values[np.argmin(counts)], 2, X_test[:,j])\n",
    "    \n",
    "#Convert array from string to float\n",
    "X_train = X_train.astype(np.float_)\n",
    "X_test = X_test.astype(np.float_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values on training data with KNNImputer\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn_imputer = KNNImputer (missing_values=np.nan, n_neighbors=4, weights=\"distance\")\n",
    "X_train = knn_imputer.fit_transform(X_train)\n",
    "\n",
    "for i in range (X_test.shape[0]):\n",
    "    x = X_test[i]\n",
    "    x.shape = (1,5000)\n",
    "    X_testr = knn_imputer.fit_transform(np.append(x,X_train,axis = 0))\n",
    "    [test_data, R] = np.split(X_testr,[1],axis = 0)\n",
    "    while i == 0:\n",
    "        test = test_data\n",
    "        break\n",
    "    while i >= 1:\n",
    "        test = np.append(test,test_data,axis = 0)\n",
    "        break\n",
    "\n",
    "for j in range (X_train.shape[1]):\n",
    "    X_train[:,j] = np.where(X_train[:,j] < 1, 0.0, X_train[:,j])\n",
    "    X_train[:,j] = np.where(X_train[:,j] >= 1, 2.0, X_train[:,j])\n",
    "    test[:,j] = np.where(test[:,j] < 1, 0.0, test[:,j])\n",
    "    test[:,j] = np.where(test[:,j] >= 1, 2.0, test[:,j])\n",
    "    X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA was not always used please see the poster for replication\n",
    "\n",
    "#standardise the data\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xn = scaler.fit_transform(X_train)\n",
    "\n",
    "#compute the covariance matrix on the standardised data\n",
    "n = Xn.shape[0]\n",
    "C = 1/(n-1) * np.dot(Xn.T,Xn)\n",
    "\n",
    "#eigendecompose the matrix  ùêÇ  into its eigenvalues and eigenvector\n",
    "import numpy.linalg as linalg\n",
    "d, V = linalg.eig(C)\n",
    "\n",
    "#sort the eigenvalues in decreasing order, re-sort the columns of the eigenvector matrix ùëâ  using the indices from the sorting of the eigenvalues\n",
    "ind = np.argsort(d)[::-1]\n",
    "d = d[ind]\n",
    "V = V[:,ind]\n",
    "\n",
    "Xn1 = np.dot(Xn,V[:,0:750])\n",
    "X_train = np.array(Xn1,dtype=float)\n",
    "\n",
    "#calculate how much of the total variance the PCs account for\n",
    "ratios_variance_explained = d/d.sum()\n",
    "va = ratios_variance_explained[0:750].sum()\n",
    "\n",
    "#transform test data\n",
    "Xn3_Xtest = np.dot(test,V[:,0:750])\n",
    "X_test = np.array(Xn3_Xtest,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Regression Model for unknown\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "knr = KNeighborsRegressor(n_neighbors=7)\n",
    "knr.fit(X_train,y_train)\n",
    "knr_predictions_training = knr.predict(X_train)\n",
    "knr_predictions_testing = knr.predict(X_test)\n",
    "\n",
    "print(\"MSE (Train):\\t%.2f\" % metrics.mean_squared_error(y_train, knr_predictions_training))\n",
    "print(\"R2 (Train):\\t%.2f\" % metrics.r2_score(y_train, knr_predictions_training))\n",
    "print(\"MSE (Test):\\t%.2f\" % metrics.mean_squared_error(y_test, knr_predictions_testing))\n",
    "print(\"R2 (Test):\\t%.2f\" % metrics.r2_score(y_test, knr_predictions_testing))\n",
    "\n",
    "delta = time.process_time() - start\n",
    "print(\"Computation Time: %f s\" %delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNet Model for unknown\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "eln =ElasticNet(alpha=0.05792901004687936, l1_ratio=0.25, max_iter=7000, random_state=73)\n",
    "eln.fit(X_train, y_train)\n",
    "predictions_training = eln.predict(X_train)\n",
    "predictions_testing = eln.predict(X_test)\n",
    "\n",
    "print(\"MSE (Train):\\t%.2f\" % metrics.mean_squared_error(y_train, predictions_training))\n",
    "print(\"R2 (Train):\\t%.2f\" % metrics.r2_score(y_train, predictions_training))\n",
    "print(\"MSE (Test):\\t%.2f\" % metrics.mean_squared_error(y_test, predictions_testing))\n",
    "print(\"R2 (Test):\\t%.2f\" % metrics.r2_score(y_test, predictions_testing))\n",
    "\n",
    "delta = (time.process_time()-start)\n",
    "print(\"Computation Time: %f s\" %delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
